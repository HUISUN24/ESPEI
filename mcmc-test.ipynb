{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from espei import run_espei\n",
    "from pycalphad import Database, binplot, equilibrium, variables as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'espei' from '/Users/sunhui/Desktop/ESPEI-NB-NI/ESPEI/espei/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import espei\n",
    "espei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mcmc.yaml') as fp:\n",
    "    mcmc_settings = yaml.safe_load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import emcee\n",
    "from espei.error_functions import calculate_zpf_error, calculate_activity_error, \\\n",
    "    calculate_non_equilibrium_thermochemical_probability, \\\n",
    "    calculate_equilibrium_thermochemical_probability, \\\n",
    "    calculate_Y_probability\n",
    "from espei.priors import PriorSpec, build_prior_specs\n",
    "from espei.utils import unpack_piecewise, optimal_parameters\n",
    "from espei.error_functions.context import setup_context\n",
    "from espei.optimizers.opt_base import OptimizerBase\n",
    "from espei.optimizers.graph import OptNode\n",
    "\n",
    "\n",
    "TRACE = 15\n",
    "\n",
    "\n",
    "class EmceeOptimizer(OptimizerBase):\n",
    "    \"\"\"\n",
    "    An optimizer using an EnsembleSampler based on Goodman and Weare [1]\n",
    "    implemented in emcee [2]\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scheduler : mappable\n",
    "        An object implementing a `map` function\n",
    "    save_interval : int\n",
    "        Interval of iterations to save the tracefile and probfile.\n",
    "    tracefile : str\n",
    "        Filename to store the trace with NumPy.save. Array has shape\n",
    "        (chains, iterations, parameters). Defaults to None.\n",
    "    probfile : str\n",
    "        filename to store the log probability with NumPy.save. Has shape (chains, iterations)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] Goodman and Weare, Ensemble Samplers with Affine Invariance. Commun. Appl. Math. Comput. Sci. 5, 65-80 (2010).\n",
    "    [2] Foreman-Mackey, Hogg, Lang, Goodman, emcee: The MCMC Hammer. Publ. Astron. Soc. Pac. 125, 306-312 (2013).\n",
    "    \"\"\"\n",
    "    def __init__(self, dbf, scheduler=None):\n",
    "        super(EmceeOptimizer, self).__init__(dbf)\n",
    "        self.scheduler = scheduler\n",
    "        self.save_interval = 1\n",
    "        # These are set by the _fit method\n",
    "        self.sampler = None\n",
    "        self.tracefile = None\n",
    "        self.probfile = None\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_new_chains(params, chains_per_parameter, std_deviation, deterministic=True):\n",
    "        \"\"\"\n",
    "        Return an array of num_samples from a Gaussian distribution about each parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            1D array of initial parameters that will be the mean of the distribution.\n",
    "        num_samples : int\n",
    "            Number of chains to initialize.\n",
    "        chains_per_parameter : int\n",
    "            number of chains for each parameter. Must be an even integer greater or\n",
    "            equal to 2. Defaults to 2.\n",
    "        std_deviation : float\n",
    "            Fractional standard deviation of the parameters to use for initialization.\n",
    "        deterministic : bool\n",
    "            True if the parameters should be generated deterministically.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Parameters are sampled from ``normal(loc=param, scale=param*std_deviation)``.\n",
    "        A parameter of zero will produce a standard deviation of zero and\n",
    "        therefore only zeros will be sampled. This will break emcee's\n",
    "        StretchMove for this parameter and only zeros will be selected.\n",
    "\n",
    "        \"\"\"\n",
    "        logging.log(TRACE, 'Initial parameters: {}'.format(params))\n",
    "        params = np.array(params)\n",
    "        num_zero_params = np.nonzero(params == 0)[0].size\n",
    "        if num_zero_params > 0:\n",
    "            logging.warning(f\"{num_zero_params} initial parameter{' is' if num_zero_params == 1 else 's are'} \"\n",
    "                            \"initialized to zero. The ensemble of chains for zero parameters will be all initialized \"\n",
    "                            \"to zero and all proposed values for these parameter will be zero. If possible, it's \"\n",
    "                            \"better to make a good guess at a reasonable parameter value to start with. \"\n",
    "                            \"Alternatively, you can start with a small value near zero and let the ensemble search \"\n",
    "                            \"parameter space.\")\n",
    "        nchains = params.size * chains_per_parameter\n",
    "        logging.info('Initializing {} chains with {} chains per parameter.'.format(nchains, chains_per_parameter))\n",
    "        if deterministic:\n",
    "            rng = np.random.RandomState(1769)\n",
    "        else:\n",
    "            rng = np.random.RandomState()\n",
    "        # apply a Gaussian random to each parameter with std dev of std_deviation*parameter\n",
    "        tiled_parameters = np.tile(params, (nchains, 1))\n",
    "        chains = rng.normal(tiled_parameters, np.abs(tiled_parameters * std_deviation))\n",
    "        return chains\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_chains_from_trace(restart_trace):\n",
    "        tr = restart_trace\n",
    "        walkers = tr[np.nonzero(tr)].reshape((tr.shape[0], -1, tr.shape[2]))[:, -1, :]\n",
    "        nchains = walkers.shape[0]\n",
    "        ndim = walkers.shape[1]\n",
    "        initial_parameters = walkers.mean(axis=0)\n",
    "        logging.info('Restarting from previous calculation with {} chains ({} per parameter).'.format(nchains, nchains / ndim))\n",
    "        logging.log(TRACE, 'Means of restarting parameters are {}'.format(initial_parameters))\n",
    "        logging.log(TRACE, 'Standard deviations of restarting parameters are {}'.format(walkers.std(axis=0)))\n",
    "        return walkers\n",
    "\n",
    "    @staticmethod\n",
    "    def get_priors(prior, symbols, params):\n",
    "        \"\"\"\n",
    "        Build priors for a particular set of fitting symbols and initial parameters.\n",
    "        Returns a dict that should be used to update the context.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prior : dict or PriorSpec or None\n",
    "            Prior to initialize. See the docs on\n",
    "        symbols : list of str\n",
    "            List of symbols that will be fit\n",
    "        params : list of float\n",
    "            List of parameter values corresponding to the symbols. These should\n",
    "            be the initial parameters that the priors will be based off of.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(prior, dict):\n",
    "            logging.info('Initializing a {} prior for the parameters.'.format(prior['name']))\n",
    "        elif isinstance(prior, PriorSpec):\n",
    "            logging.info('Initializing a {} prior for the parameters.'.format(prior.name))\n",
    "        elif prior is None:\n",
    "            prior = {'name': 'zero'}\n",
    "        prior_specs = build_prior_specs(prior, params)\n",
    "        rv_priors = []\n",
    "        for spec, param, fit_symbol in zip(prior_specs, params, symbols):\n",
    "            if isinstance(spec, PriorSpec):\n",
    "                logging.debug('Initializing a {} prior for {} with parameters: {}.'.format(spec.name, fit_symbol, spec.parameters))\n",
    "                rv_priors.append(spec.get_prior(param))\n",
    "            elif hasattr(spec, \"logpdf\"):\n",
    "                logging.debug('Using a user-specified prior for {}.'.format(fit_symbol))\n",
    "                rv_priors.append(spec)\n",
    "        return {'prior_rvs': rv_priors}\n",
    "\n",
    "    def save_sampler_state(self):\n",
    "        \"\"\"\n",
    "        Convenience function that saves the trace and lnprob if\n",
    "        they haven't been set to None by the user.\n",
    "\n",
    "        Requires that the sampler attribute be set.\n",
    "        \"\"\"\n",
    "        tr = self.tracefile\n",
    "        if tr is not None:\n",
    "            logging.log(TRACE, 'Writing trace to {}'.format(tr))\n",
    "            np.save(tr, self.sampler.chain)\n",
    "        prob = self.probfile\n",
    "        if prob is not None:\n",
    "            logging.log(TRACE, 'Writing lnprob to {}'.format(prob))\n",
    "            np.save(prob, self.sampler.lnprobability)\n",
    "\n",
    "    def do_sampling(self, chains, iterations):\n",
    "        progbar_width = 30\n",
    "        logging.info('Running MCMC for {} iterations.'.format(iterations))\n",
    "        try:\n",
    "            for i, result in enumerate(self.sampler.sample(chains, iterations=iterations)):\n",
    "                # progress bar\n",
    "                if (i + 1) % self.save_interval == 0:\n",
    "                    self.save_sampler_state()\n",
    "                    logging.log(TRACE, 'Acceptance ratios for parameters: {}'.format(self.sampler.acceptance_fraction))\n",
    "                n = int((progbar_width + 1) * float(i) / iterations)\n",
    "                logging.info(\"\\r[{0}{1}] ({2} of {3})\\n\".format('#' * n, ' ' * (progbar_width - n), i + 1, iterations))\n",
    "            n = int((progbar_width + 1) * float(i + 1) / iterations)\n",
    "            logging.info(\"\\r[{0}{1}] ({2} of {3})\\n\".format('#' * n, ' ' * (progbar_width - n), i + 1, iterations))\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        logging.info('MCMC complete.')\n",
    "        self.save_sampler_state()\n",
    "\n",
    "    def _fit(self, symbols, ds, prior=None, iterations=1000,\n",
    "             chains_per_parameter=2, chain_std_deviation=0.1, deterministic=True,\n",
    "             restart_trace=None, tracefile=None, probfile=None,\n",
    "             mcmc_data_weights=None, approximate_equilibrium=False,\n",
    "             ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        symbols : list of str\n",
    "        ds : PickleableTinyDB\n",
    "        prior : str\n",
    "            Prior to use to generate priors. Defaults to 'zero', which keeps\n",
    "            backwards compatibility. Can currently choose 'normal', 'uniform',\n",
    "            'triangular', or 'zero'.\n",
    "        iterations : int\n",
    "            Number of iterations to calculate in MCMC. Default is 1000.\n",
    "        chains_per_parameter : int\n",
    "            number of chains for each parameter. Must be an even integer greater\n",
    "            or equal to 2. Defaults to 2.\n",
    "        chain_std_deviation : float\n",
    "            Standard deviation of normal for parameter initialization as a\n",
    "            fraction of each parameter. Must be greater than 0. Defaults to 0.1.\n",
    "        deterministic : bool\n",
    "            If True, the emcee sampler will be seeded to give deterministic sampling\n",
    "            draws. This will ensure that the runs with the exact same database,\n",
    "            chains_per_parameter, and chain_std_deviation (or restart_trace) will\n",
    "            produce exactly the same results.\n",
    "        restart_trace : np.ndarray\n",
    "            ndarray of the previous trace. Should have shape (chains, iterations, parameters)\n",
    "        tracefile : str\n",
    "            filename to store the trace with NumPy.save. Array has shape\n",
    "            (chains, iterations, parameters)\n",
    "        probfile : str\n",
    "            filename to store the log probability with NumPy.save. Has shape (chains, iterations)\n",
    "        mcmc_data_weights : dict\n",
    "            Dictionary of weights for each data type, e.g. {'ZPF': 20, 'HM': 2}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        OptNode\n",
    "\n",
    "        \"\"\"\n",
    "        cbs = self.scheduler is None\n",
    "        ctx = setup_context(self.dbf, ds, symbols, data_weights=mcmc_data_weights, make_callables=cbs)\n",
    "        symbols_to_fit = ctx['symbols_to_fit']\n",
    "        initial_guess = np.array([unpack_piecewise(self.dbf.symbols[s]) for s in symbols_to_fit])\n",
    "\n",
    "        prior_dict = self.get_priors(prior, symbols_to_fit, initial_guess)\n",
    "        ctx.update(prior_dict)\n",
    "        ctx['zpf_kwargs']['approximate_equilibrium'] = approximate_equilibrium\n",
    "        ctx['equilibrium_thermochemical_kwargs']['approximate_equilibrium'] = approximate_equilibrium\n",
    "        # Run the initial parameters for guessing purposes:\n",
    "        logging.log(TRACE, \"Probability for initial parameters\")\n",
    "        self.predict(initial_guess, **ctx)\n",
    "        if restart_trace is not None:\n",
    "            chains = self.initialize_chains_from_trace(restart_trace)\n",
    "            # TODO: check that the shape is valid with the existing parameters\n",
    "        else:\n",
    "            chains = self.initialize_new_chains(initial_guess, chains_per_parameter, chain_std_deviation, deterministic)\n",
    "        sampler = emcee.EnsembleSampler(chains.shape[0], initial_guess.size, self.predict, kwargs=ctx, pool=self.scheduler)\n",
    "        if deterministic:\n",
    "            from espei.rstate import numpy_rstate\n",
    "            sampler.random_state = numpy_rstate\n",
    "            logging.info('Using a deterministic ensemble sampler.')\n",
    "        self.sampler = sampler\n",
    "        self.tracefile = tracefile\n",
    "        self.probfile = probfile\n",
    "        # Run the MCMC simulation\n",
    "        self.do_sampling(chains, iterations)\n",
    "\n",
    "        # Post process\n",
    "        optimal_params = optimal_parameters(sampler.chain, sampler.lnprobability)\n",
    "        logging.log(TRACE, 'Initial parameters: {}'.format(initial_guess))\n",
    "        logging.log(TRACE, 'Optimal parameters: {}'.format(optimal_params))\n",
    "        logging.log(TRACE, 'Change in parameters: {}'.format(np.abs(initial_guess - optimal_params) / initial_guess))\n",
    "        parameters = dict(zip(symbols_to_fit, optimal_params))\n",
    "        return OptNode(parameters, ds)\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(params, **ctx):\n",
    "        \"\"\"\n",
    "        Calculate lnprob = lnlike + lnprior\n",
    "        \"\"\"\n",
    "        logging.debug('Parameters - {}'.format(params))\n",
    "        # lnprior\n",
    "        prior_rvs = ctx['prior_rvs']\n",
    "        lnprior_multivariate = [rv.logpdf(theta) for rv, theta in zip(prior_rvs, params)]\n",
    "        logging.debug('Priors: {}'.format(lnprior_multivariate))\n",
    "        lnprior = np.sum(lnprior_multivariate)\n",
    "        if np.isneginf(lnprior):\n",
    "            # It doesn't matter what the likelihood is. We can skip calculating it to save time.\n",
    "            logging.log(TRACE, 'Proposal - lnprior: {:0.4f}, lnlike: {}, lnprob: {:0.4f}'.format(lnprior, np.nan, lnprior))\n",
    "            return lnprior\n",
    "\n",
    "        # lnlike\n",
    "        parameters = {param_name: param for param_name, param in zip(ctx['symbols_to_fit'], params)}\n",
    "        zpf_kwargs = ctx.get('zpf_kwargs')\n",
    "        activity_kwargs = ctx.get('activity_kwargs')\n",
    "        Y_kwargs = ctx.get('Y_kwargs')\n",
    "        non_equilibrium_thermochemical_kwargs = ctx.get('thermochemical_kwargs')\n",
    "        equilibrium_thermochemical_kwargs = ctx.get('equilibrium_thermochemical_kwargs')\n",
    "        starttime = time.time()\n",
    "        if zpf_kwargs is not None:\n",
    "            \n",
    "            try:\n",
    "                multi_phase_error = calculate_zpf_error(parameters=np.array(params), **zpf_kwargs)\n",
    "            except (ValueError, np.linalg.LinAlgError) as e:\n",
    "                raise e\n",
    "                print(e)\n",
    "                multi_phase_error = -np.inf\n",
    "        else:\n",
    "            multi_phase_error = 0\n",
    "        if equilibrium_thermochemical_kwargs is not None:\n",
    "            \n",
    "            eq_thermochemical_prob = calculate_equilibrium_thermochemical_probability(parameters=np.array(params), **equilibrium_thermochemical_kwargs)\n",
    "        else:\n",
    "            eq_thermochemical_prob = 0\n",
    "        if activity_kwargs is not None:\n",
    "            \n",
    "            actvity_error = calculate_activity_error(parameters=parameters, **activity_kwargs)\n",
    "        else:\n",
    "            actvity_error = 0\n",
    "        if Y_kwargs is not None:\n",
    "            print(parameters)\n",
    "            Y_prob = calculate_Y_probability(parameters=parameters,**Y_kwargs)\n",
    "        else:\n",
    "            Y_prob = 0\n",
    "        if non_equilibrium_thermochemical_kwargs is not None:\n",
    "            \n",
    "            non_eq_thermochemical_prob = calculate_non_equilibrium_thermochemical_probability(parameters=np.array(params), **non_equilibrium_thermochemical_kwargs)\n",
    "        else:\n",
    "            non_eq_thermochemical_prob = 0\n",
    "        total_error = multi_phase_error + eq_thermochemical_prob + non_eq_thermochemical_prob + actvity_error + Y_prob\n",
    "        logging.log(TRACE, f'Likelihood - {time.time() - starttime:0.2f}s - Non-equilibrium thermochemical: {non_eq_thermochemical_prob:0.3f}. Equilibrium thermochemical: {eq_thermochemical_prob:0.3f}. ZPF: {multi_phase_error:0.3f}. Activity: {actvity_error:0.3f}. Site_fractions:{Y_prob:0.3f}. Total: {total_error:0.3f}.')\n",
    "        lnlike = np.array(total_error, dtype=np.float64)\n",
    "\n",
    "        lnprob = lnprior + lnlike\n",
    "        logging.log(TRACE, 'Proposal - lnprior: {:0.4f}, lnlike: {:0.4f}, lnprob: {:0.4f}'.format(lnprior, lnlike, lnprob))\n",
    "        return lnprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import multiprocessing\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import dask\n",
    "import distributed\n",
    "import sympy\n",
    "import symengine\n",
    "import emcee\n",
    "import pycalphad\n",
    "from pycalphad import Database\n",
    "\n",
    "import espei\n",
    "from espei.validation import schema\n",
    "from espei import generate_parameters\n",
    "from espei.utils import ImmediateClient, get_dask_config_paths, database_symbols_to_fit\n",
    "from espei.datasets import DatasetError, load_datasets, recursive_glob, apply_tags, add_ideal_exclusions\n",
    "# from espei.optimizers.opt_mcmc import EmceeOptimizer\n",
    "\n",
    "TRACE = 15  # TRACE logging level|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=__doc__)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--input\", \"--in\",\n",
    "    default=None,\n",
    "    help=\"Input file for the run. Should be either a `YAML` or `JSON` file.\"\n",
    "    )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--check-datasets\",\n",
    "    metavar=\"PATH\",\n",
    "    default=None,\n",
    "    help=\"Check input datasets at the path. Does not run ESPEI.\")\n",
    "\n",
    "parser.add_argument(\"--version\", \"-v\", action='version',\n",
    "                    version='%(prog)s version '+str(espei.__version__))\n",
    "\n",
    "\n",
    "def log_version_info():\n",
    "    \"\"\"Print version info to the log\"\"\"\n",
    "    logging.info('espei version       ' + str(espei.__version__))\n",
    "    logging.debug('pycalphad version   ' + str(pycalphad.__version__))\n",
    "    logging.debug('dask version        ' + str(dask.__version__))\n",
    "    logging.debug('distributed version ' + str(distributed.__version__))\n",
    "    logging.debug('sympy version       ' + str(sympy.__version__))\n",
    "    logging.debug('symengine version   ' + str(symengine.__version__))\n",
    "    logging.debug('emcee version       ' + str(emcee.__version__))\n",
    "    logging.info(\"If you use ESPEI for work presented in a publication, we ask that you cite the following paper:\\n    {}\".format(espei.__citation__))\n",
    "\n",
    "def get_dask_config_paths():\n",
    "    candidates = dask.config.paths\n",
    "    file_paths = []\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                file_paths.extend(sorted([\n",
    "                    os.path.join(path, p)\n",
    "                    for p in os.listdir(path)\n",
    "                    if os.path.splitext(p)[1].lower() in ('.json', '.yaml', '.yml')\n",
    "                ]))\n",
    "            else:\n",
    "                file_paths.append(path)\n",
    "    return file_paths\n",
    "\n",
    "def _raise_dask_work_stealing():\n",
    "    \"\"\"\n",
    "    Raise if work stealing is turn on in dask\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    ValueError\n",
    "\n",
    "    \"\"\"\n",
    "    import distributed\n",
    "    has_work_stealing = distributed.config['distributed']['scheduler']['work-stealing']\n",
    "    if has_work_stealing:\n",
    "        raise ValueError(\"The parameter 'work-stealing' is on in dask. Enabling this parameter causes some instability. \"\n",
    "            \"Set 'distributed.scheduler.work-stealing: False' in your dask configuration. \"\n",
    "            \"Configuration files on this machine are:\\n{} (latter files have priority).\\n\"\n",
    "            \"See the example at http://espei.org/en/latest/installation.html#configuration for more.\".format(get_dask_config_paths()))\n",
    "\n",
    "\n",
    "def get_run_settings(input_dict):\n",
    "    \"\"\"\n",
    "    Validate settings from a dict of possible input.\n",
    "\n",
    "    Performs the following actions:\n",
    "    1. Normalize (apply defaults)\n",
    "    2. Validate against the schema\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dict : dict\n",
    "        Dictionary of input settings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Validated run settings\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "    \"\"\"\n",
    "    run_settings = schema.normalized(input_dict)\n",
    "\n",
    "    # can't have chain_std_deviation and chains_per_parameter defaults with restart_trace\n",
    "    if run_settings.get('mcmc') is not None:\n",
    "            if run_settings['mcmc'].get('restart_trace') is None:\n",
    "                run_settings['mcmc']['chains_per_parameter'] = run_settings['mcmc'].get('chains_per_parameter', 2)\n",
    "                run_settings['mcmc']['chain_std_deviation'] = run_settings['mcmc'].get('chain_std_deviation', 0.1)\n",
    "    if not schema.validate(run_settings):\n",
    "        raise ValueError(schema.errors)\n",
    "    return run_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_espei(run_settings):\n",
    "    \"\"\"Wrapper around the ESPEI fitting procedure, taking only a settings dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_settings : dict\n",
    "        Dictionary of input settings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Either a Database (for generate parameters only) or a tuple of (Database, sampler)\n",
    "    \"\"\"\n",
    "    run_settings = get_run_settings(run_settings)\n",
    "    system_settings = run_settings['system']\n",
    "    output_settings = run_settings['output']\n",
    "    generate_parameters_settings = run_settings.get('generate_parameters')\n",
    "    mcmc_settings = run_settings.get('mcmc')\n",
    "\n",
    "    # handle verbosity\n",
    "    verbosity = {\n",
    "        0: logging.WARNING,\n",
    "        1: logging.INFO,\n",
    "        2: TRACE,\n",
    "        3: logging.DEBUG\n",
    "    }\n",
    "    logging.basicConfig(level=verbosity[output_settings['verbosity']], filename=output_settings['logfile'])\n",
    "\n",
    "    log_version_info()\n",
    "\n",
    "    # load datasets and handle i/o\n",
    "    logging.log(TRACE, 'Loading and checking datasets.')\n",
    "    dataset_path = system_settings['datasets']\n",
    "    datasets = load_datasets(sorted(recursive_glob(dataset_path, '*.json')))\n",
    "    if len(datasets.all()) == 0:\n",
    "        logging.warning('No datasets were found in the path {}. This should be a directory containing dataset files ending in `.json`.'.format(dataset_path))\n",
    "    apply_tags(datasets, system_settings.get('tags', dict()))\n",
    "    add_ideal_exclusions(datasets)\n",
    "    logging.log(TRACE, 'Finished checking datasets')\n",
    "\n",
    "    with open(system_settings['phase_models']) as fp:\n",
    "        phase_models = json.load(fp)\n",
    "\n",
    "    if generate_parameters_settings is not None:\n",
    "        refdata = generate_parameters_settings['ref_state']\n",
    "        excess_model = generate_parameters_settings['excess_model']\n",
    "        ridge_alpha = generate_parameters_settings['ridge_alpha']\n",
    "        aicc_penalty = generate_parameters_settings['aicc_penalty_factor']\n",
    "        input_dbf = generate_parameters_settings.get('input_db', None)\n",
    "        if input_dbf is not None:\n",
    "            input_dbf = Database(input_dbf)\n",
    "        dbf = generate_parameters(phase_models, datasets, refdata, excess_model,\n",
    "                                  ridge_alpha=ridge_alpha, dbf=input_dbf,\n",
    "                                  aicc_penalty_factor=aicc_penalty,)\n",
    "        dbf.to_file(output_settings['output_db'], if_exists='overwrite')\n",
    "\n",
    "    if mcmc_settings is not None:\n",
    "        tracefile = output_settings['tracefile']\n",
    "        probfile = output_settings['probfile']\n",
    "        # Set trace and prob files to None if specified by the user.\n",
    "        if tracefile == 'None':\n",
    "            tracefile = None\n",
    "        if probfile == 'None':\n",
    "            probfile = None\n",
    "        # check that the MCMC output files do not already exist\n",
    "        # only matters if we are actually running MCMC\n",
    "        if tracefile is not None and os.path.exists(tracefile):\n",
    "            raise OSError('Tracefile \"{}\" exists and would be overwritten by a new run. Use the ``output.tracefile`` setting to set a different name.'.format(tracefile))\n",
    "        if probfile is not None and os.path.exists(probfile):\n",
    "            raise OSError('Probfile \"{}\" exists and would be overwritten by a new run. Use the ``output.probfile`` setting to set a different name.'.format(probfile))\n",
    "\n",
    "        # scheduler setup\n",
    "        if mcmc_settings['scheduler'] == 'dask':\n",
    "            _raise_dask_work_stealing()  # check for work-stealing\n",
    "            from distributed import LocalCluster\n",
    "            cores = mcmc_settings.get('cores', multiprocessing.cpu_count())\n",
    "            if (cores > multiprocessing.cpu_count()):\n",
    "                cores = multiprocessing.cpu_count()\n",
    "                logging.warning(\"The number of cores chosen is larger than available. \"\n",
    "                                \"Defaulting to run on the {} available cores.\".format(cores))\n",
    "            # TODO: make dask-scheduler-verbosity a YAML input so that users can debug. Should have the same log levels as verbosity\n",
    "            scheduler = LocalCluster(n_workers=cores, threads_per_worker=1, processes=True, memory_limit=0)\n",
    "            client = ImmediateClient(scheduler)\n",
    "            client.run(logging.basicConfig, level=verbosity[output_settings['verbosity']], filename=output_settings['logfile'])\n",
    "            logging.info(\"Running with dask scheduler: %s [%s cores]\" % (scheduler, sum(client.ncores().values())))\n",
    "            try:\n",
    "                bokeh_server_info = client.scheduler_info()['services']['bokeh']\n",
    "                logging.info(\"bokeh server for dask scheduler at localhost:{}\".format(bokeh_server_info))\n",
    "            except KeyError:\n",
    "                logging.info(\"Install bokeh to use the dask bokeh server.\")\n",
    "        elif mcmc_settings['scheduler'] == 'None' or mcmc_settings['scheduler'] is None:\n",
    "            client = None\n",
    "            logging.info(\"Not using a parallel scheduler. ESPEI is running MCMC on a single core.\")\n",
    "        else: # we were passed a scheduler file name\n",
    "            _raise_dask_work_stealing()  # check for work-stealing\n",
    "            client = ImmediateClient(scheduler_file=mcmc_settings['scheduler'])\n",
    "            client.run(logging.basicConfig, level=verbosity[output_settings['verbosity']], filename=output_settings['logfile'])\n",
    "            logging.info(\"Running with dask scheduler: %s [%s cores]\" % (client.scheduler, sum(client.ncores().values())))\n",
    "\n",
    "        # get a Database\n",
    "        if mcmc_settings.get('input_db'):\n",
    "            dbf = Database(mcmc_settings.get('input_db'))\n",
    "\n",
    "        # load the restart trace if needed\n",
    "        if mcmc_settings.get('restart_trace'):\n",
    "            restart_trace = np.load(mcmc_settings.get('restart_trace'))\n",
    "        else:\n",
    "            restart_trace = None\n",
    "\n",
    "        # load the remaining mcmc fitting parameters\n",
    "        iterations = mcmc_settings.get('iterations')\n",
    "        save_interval = mcmc_settings.get('save_interval')\n",
    "        chains_per_parameter = mcmc_settings.get('chains_per_parameter')\n",
    "        chain_std_deviation = mcmc_settings.get('chain_std_deviation')\n",
    "        deterministic = mcmc_settings.get('deterministic')\n",
    "        prior = mcmc_settings.get('prior')\n",
    "        data_weights = mcmc_settings.get('data_weights')\n",
    "        syms = mcmc_settings.get('symbols')\n",
    "        approximate_equilibrium = mcmc_settings.get('approximate_equilibrium')\n",
    "\n",
    "        # set up and run the EmceeOptimizer\n",
    "        optimizer = EmceeOptimizer(dbf, scheduler=client)\n",
    "        optimizer.save_interval = save_interval\n",
    "        all_symbols = syms if syms is not None else database_symbols_to_fit(dbf)\n",
    "        optimizer.fit(all_symbols, datasets, prior=prior, iterations=iterations,\n",
    "                      chains_per_parameter=chains_per_parameter,\n",
    "                      chain_std_deviation=chain_std_deviation,\n",
    "                      deterministic=deterministic, restart_trace=restart_trace,\n",
    "                      tracefile=tracefile, probfile=probfile,\n",
    "                      mcmc_data_weights=data_weights,\n",
    "                      approximate_equilibrium=approximate_equilibrium,\n",
    "                      )\n",
    "        optimizer.commit()\n",
    "\n",
    "        optimizer.dbf.to_file(output_settings['output_db'], if_exists='overwrite')\n",
    "        # close the scheduler, if possible\n",
    "        if hasattr(client, 'close'):\n",
    "                client.close()\n",
    "        return optimizer.dbf, optimizer.sampler\n",
    "    return dbf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:espei version       0.7.8+0.g3f1d679.dirty\n",
      "INFO:root:If you use ESPEI for work presented in a publication, we ask that you cite the following paper:\n",
      "    B. Bocklund, R. Otis, A. Egorov, A. Obaied, I. Roslyakova, Z.-K. Liu, ESPEI for efficient thermodynamic database development, modification, and uncertainty quantification: application to Cu-Mg, MRS Commun. (2019) 1-10. doi:10.1557/mrc.2019.59.\n",
      "TRACE:root:Loading and checking datasets.\n",
      "TRACE:root:Finished checking datasets\n",
      "INFO:root:Not using a parallel scheduler. ESPEI is running MCMC on a single core.\n",
      "INFO:root:Fitting 28 degrees of freedom.\n",
      "TRACE:root:Building phase models (this may take some time)\n",
      "TRACE:root:Finished building phase models (3.02s)\n",
      "TRACE:root:Getting non-equilibrium thermochemical data (this may take some time)\n",
      "TRACE:root:Finished getting non-equilibrium thermochemical data (0.00s)\n",
      "TRACE:root:Getting equilibrium thermochemical data (this may take some time)\n",
      "TRACE:root:Finished getting equilibrium thermochemical data (0.00s)\n",
      "TRACE:root:Getting ZPF data (this may take some time)\n",
      "TRACE:root:Finished getting ZPF data (17.85s)\n",
      "INFO:root:Initializing a zero prior for the parameters.\n",
      "TRACE:root:Probability for initial parameters\n",
      "TRACE:root:Likelihood - 24.41s - Non-equilibrium thermochemical: 0.000. Equilibrium thermochemical: 0.000. ZPF: -45167.207. Activity: 0.000. Site_fractions:-35283.264. Total: -80450.472.\n",
      "TRACE:root:Proposal - lnprior: 0.0000, lnlike: -80450.4719, lnprob: -80450.4719\n",
      "TRACE:root:Initial parameters: [ 1.49512565e+05 -3.20610377e+04  9.37541385e+03 -2.98068557e+04\n",
      "  1.01584916e+05  2.90924066e+05  6.69991309e+05 -2.91239910e+05\n",
      " -2.65455196e+05  2.49022826e+05 -5.07910218e+01 -2.24643627e+01\n",
      " -1.55051034e+05 -8.75431022e+00 -1.07523197e+05 -7.08997104e+05\n",
      " -1.39170128e+05 -6.53776857e+01 -1.79594597e+05 -6.19436219e+04\n",
      "  1.49767545e+05 -2.75165724e+05  0.00000000e+00 -1.62332712e+05\n",
      " -5.40291265e+05 -3.32521025e+04  0.00000000e+00  5.68044201e+04]\n",
      "WARNING:root:2 initial parameters are initialized to zero. The ensemble of chains for zero parameters will be all initialized to zero and all proposed values for these parameter will be zero. If possible, it's better to make a good guess at a reasonable parameter value to start with. Alternatively, you can start with a small value near zero and let the ensemble search parameter space.\n",
      "INFO:root:Initializing 56 chains with 2 chains per parameter.\n",
      "INFO:root:Using a deterministic ensemble sampler.\n",
      "INFO:root:Running MCMC for 100 iterations.\n"
     ]
    }
   ],
   "source": [
    "dbf_mcmc, sampler = run_espei(mcmc_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:espei-new]",
   "language": "python",
   "name": "conda-env-espei-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
